# -*- coding: utf-8 -*-
"""Copie_de_Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TJjYSr5Ot0acqI_tq5aIw7pCiBnyISpd
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

from google.colab import files
uploaded = files.upload()

df_train = pd.read_csv("ECG5000_TRAIN.txt", delim_whitespace=True, header=None)
df_test = pd.read_csv("ECG5000_TEST.txt", delim_whitespace=True, header=None)
df = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)
df[0] = df[0].apply(lambda x: 0 if x == 1 else 1)  # 0 = normal, 1 = anomalie


print(df.shape)
print(df.head())

X = df.iloc[:, 1:].values
y = df.iloc[:, 0].values

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_cnn = torch.tensor(X_scaled).float().unsqueeze(1)
X_lstm = torch.tensor(X_scaled).float().unsqueeze(2)
y_tensor = torch.tensor(y).long()
assert y_tensor.min() >= 0 and y_tensor.max() <= 1, "Erreur : y_tensor contient des valeurs hors [0, 1]"

print("Valeurs uniques dans y :", y_tensor.unique())


Xc_train, Xc_test, Xl_train, Xl_test, y_train, y_test = train_test_split(
    X_cnn, X_lstm, y_tensor, test_size=0.2, random_state=42, stratify=y)

batch_size = 64
train_loader_cnn = DataLoader(TensorDataset(Xc_train, y_train), batch_size=batch_size, shuffle=True)
test_loader_cnn = DataLoader(TensorDataset(Xc_test, y_test), batch_size=batch_size)
train_loader_lstm = DataLoader(TensorDataset(Xl_train, y_train), batch_size=batch_size, shuffle=True)
test_loader_lstm = DataLoader(TensorDataset(Xl_test, y_test), batch_size=batch_size)

class CNN1D(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv1d(1, 32, kernel_size=5, padding=2)
        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.AdaptiveMaxPool1d(1)
        self.drop = nn.Dropout(0.3)
        self.fc = nn.Linear(64, 2)
    def forward(self, x):
        print("Input shape:", x.shape)  # Add print statement for input shape
        x = torch.relu(self.conv1(x))
        print("After conv1:", x.shape)  # Add print statement after conv1
        x = torch.relu(self.conv2(x))
        print("After conv2:", x.shape)  # Add print statement after conv2
        x = self.pool(x).squeeze(-1)
        print("After pool:", x.shape)  # Add print statement after pool
        x = self.drop(x)
        print("After dropout:", x.shape)  # Add print statement after dropout
        return self.fc(x)

class LSTM1D(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(input_size=1, hidden_size=64, batch_first=True)
        self.drop = nn.Dropout(0.3)
        self.fc = nn.Linear(64, 2)
    def forward(self, x):
        _, (h_n, _) = self.lstm(x)
        x = self.drop(h_n[-1])
        return self.fc(x)

def train_model(model, train_loader, test_loader, epochs=100):
    model = model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)
    criterion = nn.CrossEntropyLoss()

    best_auc = 0
    best_model_state = None

    train_losses = []
    val_aucs = []

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for xb, yb in train_loader:
            xb, yb = xb.to(device), yb.to(device)

            out = model(xb)

            # üîç S√©curit√©
            assert not torch.isnan(out).any(), "Erreur : sortie du mod√®le contient NaN"
            assert out.shape[1] == 2, f"Erreur : sortie inattendue du mod√®le, shape = {out.shape}"
            assert yb.min() >= 0 and yb.max() <= 1, f"Erreur : labels hors [0, 1] : {yb.unique()}"

            loss = criterion(out, yb)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        scheduler.step()

        train_losses.append(total_loss / len(train_loader))

        # √âvaluation AUC
        model.eval()
        all_probs = []
        all_labels = []
        with torch.no_grad():
            for xb, yb in test_loader:
                probs = torch.softmax(model(xb.to(device)), dim=1)[:, 1].cpu().numpy()
                all_probs.extend(probs)
                all_labels.extend(yb.numpy())

        auc = roc_auc_score(all_labels, all_probs)
        val_aucs.append(auc)

        print(f"Epoch {epoch+1:2d} | Loss: {total_loss:.4f} | AUC: {auc:.4f}")

        if auc > best_auc:
            best_auc = auc
            best_model_state = model.state_dict()

    # Tracer les courbes
    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Train Loss')
    plt.xlabel("√âpoque")
    plt.ylabel("Loss")
    plt.title("Courbe de Loss")
    plt.grid(True)
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(val_aucs, label='AUC sur test', color='orange')
    plt.xlabel("√âpoque")
    plt.ylabel("AUC")
    plt.title("Courbe d'AUC")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

    model.load_state_dict(best_model_state)
    return model, train_losses, val_aucs

def evaluate(model, test_loader):
    model.eval()
    y_true, y_pred, y_scores = [], [], []
    with torch.no_grad():
        for xb, yb in test_loader:
            logits = model(xb.to(device))
            probs = torch.softmax(logits, dim=1)
            y_scores.extend(probs[:, 1].cpu().numpy())
            y_pred.extend(torch.argmax(probs, dim=1).cpu().numpy())
            y_true.extend(yb.numpy())

    print(classification_report(y_true, y_pred))
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title("Matrice de confusion")
    plt.xlabel("Pr√©dit")
    plt.ylabel("R√©el")
    plt.show()

    fpr, tpr, _ = roc_curve(y_true, y_scores)
    auc = roc_auc_score(y_true, y_scores)
    plt.plot(fpr, tpr, label=f"AUC = {auc:.2f}")
    plt.plot([0, 1], [0, 1], '--')
    plt.title("Courbe ROC")
    plt.xlabel("Taux de faux positifs")
    plt.ylabel("Taux de vrais positifs")
    plt.legend()
    plt.grid(True)
    plt.show()

    plt.hist([y_scores[i] for i in range(len(y_scores)) if y_true[i] == 0], bins=30, alpha=0.5, label='Classe 0 (normal)')
    plt.hist([y_scores[i] for i in range(len(y_scores)) if y_true[i] == 1], bins=30, alpha=0.5, label='Classe 1 (anomalie)')
    plt.title("Distribution des scores de probabilit√©")
    plt.xlabel("Score (proba classe 1)")
    plt.ylabel("Fr√©quence")
    plt.legend()
    plt.grid(True)
    plt.show()

def plot_training_curves(train_losses, val_aucs, title=""):
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Train Loss')
    plt.xlabel("√âpoque")
    plt.ylabel("Loss")
    plt.title("Courbe de Loss" + title)
    plt.grid(True)
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(val_aucs, label='AUC sur test', color='orange')
    plt.xlabel("√âpoque")
    plt.ylabel("AUC")
    plt.title("Courbe d'AUC" + title)
    plt.grid(True)
    plt.legend()

    plt.tight_layout()
    plt.show()

cnn = CNN1D()
cnn, cnn_losses, cnn_aucs = train_model(cnn, train_loader_cnn, test_loader_cnn)
plot_training_curves(cnn_losses, cnn_aucs, title=" (CNN)")

lstm = LSTM1D()
lstm, lstm_losses, lstm_aucs = train_model(lstm, train_loader_lstm, test_loader_lstm)
plot_training_curves(lstm_losses, lstm_aucs, title=" (LSTM)")

